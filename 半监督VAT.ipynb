{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             1         2         3         4         5         6         7  \\\n0     0.058059  0.041292  0.041770  0.030273  0.023149  0.022876  0.028087   \n1     0.058708  0.051248  0.027106  0.017211  0.014248  0.013115  0.014661   \n2     0.041497  0.041953  0.016199  0.012814  0.013885  0.017758  0.023270   \n3     0.040264  0.025661  0.030245  0.032230  0.035553  0.040689  0.048189   \n4     0.040258  0.026491  0.028075  0.030627  0.034040  0.038932  0.048109   \n...        ...       ...       ...       ...       ...       ...       ...   \n2475  0.052696  0.033388  0.034286  0.032447  0.030568  0.031593  0.036913   \n2476  0.047615  0.029783  0.040016  0.036314  0.032503  0.032559  0.038065   \n2477  0.049829  0.031640  0.040739  0.037326  0.032006  0.030997  0.037161   \n2478  0.041975  0.026444  0.027807  0.031494  0.033674  0.039239  0.048177   \n2479  0.041242  0.026460  0.030575  0.032143  0.034320  0.038118  0.046304   \n\n             8         9        10  ...       760       761       762  \\\n0     0.045512  0.053745  0.044851  ...  0.042317 -0.052999 -0.140937   \n1     0.031506  0.035307  0.031559  ...  0.098308 -0.128043 -0.098877   \n2     0.043000  0.064817  0.048953  ...  0.068439  0.000398  0.047242   \n3     0.077196  0.083134  0.077550  ...  0.020538 -0.133970 -0.055184   \n4     0.076547  0.085870  0.078065  ...  0.055476 -0.027392  0.026791   \n...        ...       ...       ...  ...       ...       ...       ...   \n2475  0.053742  0.061143  0.058118  ...  0.037147 -0.018968 -0.090092   \n2476  0.065357  0.076245  0.070071  ...  0.083917 -0.204559  0.076698   \n2477  0.059053  0.073540  0.063494  ...  0.052647  0.005318 -0.056510   \n2478  0.074252  0.090720  0.074031  ...  0.031354 -0.079182 -0.115513   \n2479  0.074748  0.084615  0.072202  ...  0.017500 -0.095259 -0.044797   \n\n           763       764       765       766       767       768  Label  \n0    -0.044338  0.071391 -0.020000 -0.211531 -0.201435  0.224799      0  \n1    -0.031098 -0.052139 -0.169000 -0.215130 -0.104537  0.156784      0  \n2     0.190759 -0.223332 -0.239000 -0.088656 -0.331340  0.067402      0  \n3     0.064948  0.041458 -0.039900 -0.113489 -0.178424  0.154598      0  \n4     0.144202  0.093995  0.022300 -0.194535 -0.162177  0.113191      0  \n...        ...       ...       ...       ...       ...       ...    ...  \n2475  0.145131  0.141736 -0.073313 -0.108215 -0.185594  0.119101      0  \n2476  0.198684 -0.078079 -0.043522 -0.176732 -0.082772  0.142243      0  \n2477  0.181856  0.029697  0.058450 -0.188488 -0.336352  0.100729      0  \n2478  0.107716  0.059545  0.066621 -0.146356 -0.086984  0.130130      0  \n2479  0.078060  0.059512 -0.010592 -0.126012 -0.091809  0.141970      0  \n\n[2480 rows x 813 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>768</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.058059</td>\n      <td>0.041292</td>\n      <td>0.041770</td>\n      <td>0.030273</td>\n      <td>0.023149</td>\n      <td>0.022876</td>\n      <td>0.028087</td>\n      <td>0.045512</td>\n      <td>0.053745</td>\n      <td>0.044851</td>\n      <td>...</td>\n      <td>0.042317</td>\n      <td>-0.052999</td>\n      <td>-0.140937</td>\n      <td>-0.044338</td>\n      <td>0.071391</td>\n      <td>-0.020000</td>\n      <td>-0.211531</td>\n      <td>-0.201435</td>\n      <td>0.224799</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.058708</td>\n      <td>0.051248</td>\n      <td>0.027106</td>\n      <td>0.017211</td>\n      <td>0.014248</td>\n      <td>0.013115</td>\n      <td>0.014661</td>\n      <td>0.031506</td>\n      <td>0.035307</td>\n      <td>0.031559</td>\n      <td>...</td>\n      <td>0.098308</td>\n      <td>-0.128043</td>\n      <td>-0.098877</td>\n      <td>-0.031098</td>\n      <td>-0.052139</td>\n      <td>-0.169000</td>\n      <td>-0.215130</td>\n      <td>-0.104537</td>\n      <td>0.156784</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.041497</td>\n      <td>0.041953</td>\n      <td>0.016199</td>\n      <td>0.012814</td>\n      <td>0.013885</td>\n      <td>0.017758</td>\n      <td>0.023270</td>\n      <td>0.043000</td>\n      <td>0.064817</td>\n      <td>0.048953</td>\n      <td>...</td>\n      <td>0.068439</td>\n      <td>0.000398</td>\n      <td>0.047242</td>\n      <td>0.190759</td>\n      <td>-0.223332</td>\n      <td>-0.239000</td>\n      <td>-0.088656</td>\n      <td>-0.331340</td>\n      <td>0.067402</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.040264</td>\n      <td>0.025661</td>\n      <td>0.030245</td>\n      <td>0.032230</td>\n      <td>0.035553</td>\n      <td>0.040689</td>\n      <td>0.048189</td>\n      <td>0.077196</td>\n      <td>0.083134</td>\n      <td>0.077550</td>\n      <td>...</td>\n      <td>0.020538</td>\n      <td>-0.133970</td>\n      <td>-0.055184</td>\n      <td>0.064948</td>\n      <td>0.041458</td>\n      <td>-0.039900</td>\n      <td>-0.113489</td>\n      <td>-0.178424</td>\n      <td>0.154598</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.040258</td>\n      <td>0.026491</td>\n      <td>0.028075</td>\n      <td>0.030627</td>\n      <td>0.034040</td>\n      <td>0.038932</td>\n      <td>0.048109</td>\n      <td>0.076547</td>\n      <td>0.085870</td>\n      <td>0.078065</td>\n      <td>...</td>\n      <td>0.055476</td>\n      <td>-0.027392</td>\n      <td>0.026791</td>\n      <td>0.144202</td>\n      <td>0.093995</td>\n      <td>0.022300</td>\n      <td>-0.194535</td>\n      <td>-0.162177</td>\n      <td>0.113191</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2475</th>\n      <td>0.052696</td>\n      <td>0.033388</td>\n      <td>0.034286</td>\n      <td>0.032447</td>\n      <td>0.030568</td>\n      <td>0.031593</td>\n      <td>0.036913</td>\n      <td>0.053742</td>\n      <td>0.061143</td>\n      <td>0.058118</td>\n      <td>...</td>\n      <td>0.037147</td>\n      <td>-0.018968</td>\n      <td>-0.090092</td>\n      <td>0.145131</td>\n      <td>0.141736</td>\n      <td>-0.073313</td>\n      <td>-0.108215</td>\n      <td>-0.185594</td>\n      <td>0.119101</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2476</th>\n      <td>0.047615</td>\n      <td>0.029783</td>\n      <td>0.040016</td>\n      <td>0.036314</td>\n      <td>0.032503</td>\n      <td>0.032559</td>\n      <td>0.038065</td>\n      <td>0.065357</td>\n      <td>0.076245</td>\n      <td>0.070071</td>\n      <td>...</td>\n      <td>0.083917</td>\n      <td>-0.204559</td>\n      <td>0.076698</td>\n      <td>0.198684</td>\n      <td>-0.078079</td>\n      <td>-0.043522</td>\n      <td>-0.176732</td>\n      <td>-0.082772</td>\n      <td>0.142243</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2477</th>\n      <td>0.049829</td>\n      <td>0.031640</td>\n      <td>0.040739</td>\n      <td>0.037326</td>\n      <td>0.032006</td>\n      <td>0.030997</td>\n      <td>0.037161</td>\n      <td>0.059053</td>\n      <td>0.073540</td>\n      <td>0.063494</td>\n      <td>...</td>\n      <td>0.052647</td>\n      <td>0.005318</td>\n      <td>-0.056510</td>\n      <td>0.181856</td>\n      <td>0.029697</td>\n      <td>0.058450</td>\n      <td>-0.188488</td>\n      <td>-0.336352</td>\n      <td>0.100729</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2478</th>\n      <td>0.041975</td>\n      <td>0.026444</td>\n      <td>0.027807</td>\n      <td>0.031494</td>\n      <td>0.033674</td>\n      <td>0.039239</td>\n      <td>0.048177</td>\n      <td>0.074252</td>\n      <td>0.090720</td>\n      <td>0.074031</td>\n      <td>...</td>\n      <td>0.031354</td>\n      <td>-0.079182</td>\n      <td>-0.115513</td>\n      <td>0.107716</td>\n      <td>0.059545</td>\n      <td>0.066621</td>\n      <td>-0.146356</td>\n      <td>-0.086984</td>\n      <td>0.130130</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2479</th>\n      <td>0.041242</td>\n      <td>0.026460</td>\n      <td>0.030575</td>\n      <td>0.032143</td>\n      <td>0.034320</td>\n      <td>0.038118</td>\n      <td>0.046304</td>\n      <td>0.074748</td>\n      <td>0.084615</td>\n      <td>0.072202</td>\n      <td>...</td>\n      <td>0.017500</td>\n      <td>-0.095259</td>\n      <td>-0.044797</td>\n      <td>0.078060</td>\n      <td>0.059512</td>\n      <td>-0.010592</td>\n      <td>-0.126012</td>\n      <td>-0.091809</td>\n      <td>0.141970</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2480 rows × 813 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_files = [\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Benign_LBP.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Benign_形态学.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Benign_过滤器.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Benign_骨架特征.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Benign_高级特征.csv'\n",
    "]\n",
    "# 如果各CSV文件有相同的样本ID列，则保留一列即可，否则需指定合并键\n",
    "merge_on_column = 'Sample_ID'  # 假设是样本ID列的名称\n",
    "data_frames = []\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path,header=None)\n",
    "    data_frames.append(df)\n",
    "Benign_df = pd.concat(data_frames, axis=1, join='outer', ignore_index=False)\n",
    "Benign_df.drop(Benign_df.columns[0], axis=1, inplace=True)\n",
    "Benign_df['Label'] = 0\n",
    "Benign_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T07:22:33.797098Z",
     "start_time": "2024-04-27T07:22:33.476454Z"
    }
   },
   "id": "a7fd57fbefedde05",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             1         2         3         4         5         6         7  \\\n0     0.064146  0.037661  0.046373  0.037845  0.029978  0.026366  0.028314   \n1     0.048680  0.030807  0.038512  0.035745  0.031022  0.030432  0.035043   \n2     0.065068  0.032137  0.044727  0.042317  0.036016  0.032534  0.032742   \n3     0.048413  0.036463  0.038469  0.031565  0.028012  0.029435  0.037031   \n4     0.051522  0.036155  0.039904  0.034481  0.030978  0.030494  0.035966   \n...        ...       ...       ...       ...       ...       ...       ...   \n2475  0.031755  0.019953  0.024758  0.030994  0.034981  0.041137  0.056205   \n2476  0.029550  0.023065  0.019590  0.022693  0.025671  0.032891  0.050739   \n2477  0.056590  0.036146  0.038090  0.033783  0.030009  0.030224  0.035295   \n2478  0.055444  0.036059  0.043130  0.036509  0.030981  0.031196  0.034568   \n2479  0.052534  0.035702  0.041593  0.035208  0.029963  0.029941  0.036255   \n\n             8         9        10  ...       760       761       762  \\\n0     0.039540  0.039764  0.033661  ...  0.045638 -0.110468 -0.121095   \n1     0.059618  0.076503  0.068363  ...  0.068195  0.007039  0.145410   \n2     0.040460  0.040994  0.036941  ...  0.029324 -0.160985 -0.101699   \n3     0.070019  0.082469  0.057339  ...  0.006476  0.050543 -0.116403   \n4     0.059969  0.066000  0.051320  ...  0.014970  0.043391 -0.114600   \n...        ...       ...       ...  ...       ...       ...       ...   \n2475  0.097056  0.121208  0.090960  ...  0.077830 -0.032965 -0.064442   \n2476  0.098898  0.133602  0.104019  ...  0.060514  0.043260 -0.200303   \n2477  0.052807  0.056211  0.047584  ...  0.052650  0.094038  0.063539   \n2478  0.052975  0.056481  0.049994  ...  0.032762 -0.071837 -0.018743   \n2479  0.060280  0.065134  0.052870  ...  0.019050  0.006829 -0.034760   \n\n           763       764       765       766       767       768  Label  \n0    -0.035262  0.069131  0.016740 -0.107784 -0.137001  0.180531      1  \n1     0.063756  0.029126  0.008450 -0.206205 -0.271929  0.176297      1  \n2     0.089310 -0.023350  0.099837 -0.179642 -0.205744  0.107194      1  \n3     0.054110 -0.103284  0.214400 -0.050038 -0.304897  0.116992      1  \n4    -0.008028 -0.032253  0.144668 -0.081111 -0.231332  0.165160      1  \n...        ...       ...       ...       ...       ...       ...    ...  \n2475  0.039278 -0.080521  0.001011 -0.169109 -0.179915  0.046537      1  \n2476  0.297538  0.045275  0.055709 -0.104880 -0.276354  0.155365      1  \n2477  0.094774 -0.020944 -0.006468 -0.192807 -0.245543  0.048387      1  \n2478  0.011795 -0.100116  0.101405 -0.145528 -0.305955  0.103323      1  \n2479  0.042489 -0.018958  0.219072 -0.089119 -0.283946  0.057813      1  \n\n[2480 rows x 813 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>768</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.064146</td>\n      <td>0.037661</td>\n      <td>0.046373</td>\n      <td>0.037845</td>\n      <td>0.029978</td>\n      <td>0.026366</td>\n      <td>0.028314</td>\n      <td>0.039540</td>\n      <td>0.039764</td>\n      <td>0.033661</td>\n      <td>...</td>\n      <td>0.045638</td>\n      <td>-0.110468</td>\n      <td>-0.121095</td>\n      <td>-0.035262</td>\n      <td>0.069131</td>\n      <td>0.016740</td>\n      <td>-0.107784</td>\n      <td>-0.137001</td>\n      <td>0.180531</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.048680</td>\n      <td>0.030807</td>\n      <td>0.038512</td>\n      <td>0.035745</td>\n      <td>0.031022</td>\n      <td>0.030432</td>\n      <td>0.035043</td>\n      <td>0.059618</td>\n      <td>0.076503</td>\n      <td>0.068363</td>\n      <td>...</td>\n      <td>0.068195</td>\n      <td>0.007039</td>\n      <td>0.145410</td>\n      <td>0.063756</td>\n      <td>0.029126</td>\n      <td>0.008450</td>\n      <td>-0.206205</td>\n      <td>-0.271929</td>\n      <td>0.176297</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.065068</td>\n      <td>0.032137</td>\n      <td>0.044727</td>\n      <td>0.042317</td>\n      <td>0.036016</td>\n      <td>0.032534</td>\n      <td>0.032742</td>\n      <td>0.040460</td>\n      <td>0.040994</td>\n      <td>0.036941</td>\n      <td>...</td>\n      <td>0.029324</td>\n      <td>-0.160985</td>\n      <td>-0.101699</td>\n      <td>0.089310</td>\n      <td>-0.023350</td>\n      <td>0.099837</td>\n      <td>-0.179642</td>\n      <td>-0.205744</td>\n      <td>0.107194</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.048413</td>\n      <td>0.036463</td>\n      <td>0.038469</td>\n      <td>0.031565</td>\n      <td>0.028012</td>\n      <td>0.029435</td>\n      <td>0.037031</td>\n      <td>0.070019</td>\n      <td>0.082469</td>\n      <td>0.057339</td>\n      <td>...</td>\n      <td>0.006476</td>\n      <td>0.050543</td>\n      <td>-0.116403</td>\n      <td>0.054110</td>\n      <td>-0.103284</td>\n      <td>0.214400</td>\n      <td>-0.050038</td>\n      <td>-0.304897</td>\n      <td>0.116992</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.051522</td>\n      <td>0.036155</td>\n      <td>0.039904</td>\n      <td>0.034481</td>\n      <td>0.030978</td>\n      <td>0.030494</td>\n      <td>0.035966</td>\n      <td>0.059969</td>\n      <td>0.066000</td>\n      <td>0.051320</td>\n      <td>...</td>\n      <td>0.014970</td>\n      <td>0.043391</td>\n      <td>-0.114600</td>\n      <td>-0.008028</td>\n      <td>-0.032253</td>\n      <td>0.144668</td>\n      <td>-0.081111</td>\n      <td>-0.231332</td>\n      <td>0.165160</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2475</th>\n      <td>0.031755</td>\n      <td>0.019953</td>\n      <td>0.024758</td>\n      <td>0.030994</td>\n      <td>0.034981</td>\n      <td>0.041137</td>\n      <td>0.056205</td>\n      <td>0.097056</td>\n      <td>0.121208</td>\n      <td>0.090960</td>\n      <td>...</td>\n      <td>0.077830</td>\n      <td>-0.032965</td>\n      <td>-0.064442</td>\n      <td>0.039278</td>\n      <td>-0.080521</td>\n      <td>0.001011</td>\n      <td>-0.169109</td>\n      <td>-0.179915</td>\n      <td>0.046537</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2476</th>\n      <td>0.029550</td>\n      <td>0.023065</td>\n      <td>0.019590</td>\n      <td>0.022693</td>\n      <td>0.025671</td>\n      <td>0.032891</td>\n      <td>0.050739</td>\n      <td>0.098898</td>\n      <td>0.133602</td>\n      <td>0.104019</td>\n      <td>...</td>\n      <td>0.060514</td>\n      <td>0.043260</td>\n      <td>-0.200303</td>\n      <td>0.297538</td>\n      <td>0.045275</td>\n      <td>0.055709</td>\n      <td>-0.104880</td>\n      <td>-0.276354</td>\n      <td>0.155365</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2477</th>\n      <td>0.056590</td>\n      <td>0.036146</td>\n      <td>0.038090</td>\n      <td>0.033783</td>\n      <td>0.030009</td>\n      <td>0.030224</td>\n      <td>0.035295</td>\n      <td>0.052807</td>\n      <td>0.056211</td>\n      <td>0.047584</td>\n      <td>...</td>\n      <td>0.052650</td>\n      <td>0.094038</td>\n      <td>0.063539</td>\n      <td>0.094774</td>\n      <td>-0.020944</td>\n      <td>-0.006468</td>\n      <td>-0.192807</td>\n      <td>-0.245543</td>\n      <td>0.048387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2478</th>\n      <td>0.055444</td>\n      <td>0.036059</td>\n      <td>0.043130</td>\n      <td>0.036509</td>\n      <td>0.030981</td>\n      <td>0.031196</td>\n      <td>0.034568</td>\n      <td>0.052975</td>\n      <td>0.056481</td>\n      <td>0.049994</td>\n      <td>...</td>\n      <td>0.032762</td>\n      <td>-0.071837</td>\n      <td>-0.018743</td>\n      <td>0.011795</td>\n      <td>-0.100116</td>\n      <td>0.101405</td>\n      <td>-0.145528</td>\n      <td>-0.305955</td>\n      <td>0.103323</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2479</th>\n      <td>0.052534</td>\n      <td>0.035702</td>\n      <td>0.041593</td>\n      <td>0.035208</td>\n      <td>0.029963</td>\n      <td>0.029941</td>\n      <td>0.036255</td>\n      <td>0.060280</td>\n      <td>0.065134</td>\n      <td>0.052870</td>\n      <td>...</td>\n      <td>0.019050</td>\n      <td>0.006829</td>\n      <td>-0.034760</td>\n      <td>0.042489</td>\n      <td>-0.018958</td>\n      <td>0.219072</td>\n      <td>-0.089119</td>\n      <td>-0.283946</td>\n      <td>0.057813</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2480 rows × 813 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_files = [\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Malignant_LBP.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Malignant_形态学.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Malignant_过滤器.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Malignant_骨架特征.csv',\n",
    "    'C:\\\\Users\\\\MYZ\\\\PycharmProjects\\\\毕设实验\\\\Malignant_高级特征.csv'\n",
    "]\n",
    "# 如果各CSV文件有相同的样本ID列，则保留一列即可，否则需指定合并键\n",
    "data_frames = []\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path,header=None)\n",
    "    data_frames.append(df)\n",
    "Malignant_df = pd.concat(data_frames, axis=1, join='outer', ignore_index=False)\n",
    "Malignant_df.drop(Malignant_df.columns[0], axis=1, inplace=True)\n",
    "Malignant_df['Label'] = 1\n",
    "Malignant_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T07:22:36.864123Z",
     "start_time": "2024-04-27T07:22:36.584693Z"
    }
   },
   "id": "a2cf1c4b12f01639",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             1         2         3         4         5         6         7  \\\n0     0.058059  0.041292  0.041770  0.030273  0.023149  0.022876  0.028087   \n1     0.058708  0.051248  0.027106  0.017211  0.014248  0.013115  0.014661   \n2     0.041497  0.041953  0.016199  0.012814  0.013885  0.017758  0.023270   \n3     0.040264  0.025661  0.030245  0.032230  0.035553  0.040689  0.048189   \n4     0.040258  0.026491  0.028075  0.030627  0.034040  0.038932  0.048109   \n...        ...       ...       ...       ...       ...       ...       ...   \n4955  0.031755  0.019953  0.024758  0.030994  0.034981  0.041137  0.056205   \n4956  0.029550  0.023065  0.019590  0.022693  0.025671  0.032891  0.050739   \n4957  0.056590  0.036146  0.038090  0.033783  0.030009  0.030224  0.035295   \n4958  0.055444  0.036059  0.043130  0.036509  0.030981  0.031196  0.034568   \n4959  0.052534  0.035702  0.041593  0.035208  0.029963  0.029941  0.036255   \n\n             8         9        10  ...       760       761       762  \\\n0     0.045512  0.053745  0.044851  ...  0.042317 -0.052999 -0.140937   \n1     0.031506  0.035307  0.031559  ...  0.098308 -0.128043 -0.098877   \n2     0.043000  0.064817  0.048953  ...  0.068439  0.000398  0.047242   \n3     0.077196  0.083134  0.077550  ...  0.020538 -0.133970 -0.055184   \n4     0.076547  0.085870  0.078065  ...  0.055476 -0.027392  0.026791   \n...        ...       ...       ...  ...       ...       ...       ...   \n4955  0.097056  0.121208  0.090960  ...  0.077830 -0.032965 -0.064442   \n4956  0.098898  0.133602  0.104019  ...  0.060514  0.043260 -0.200303   \n4957  0.052807  0.056211  0.047584  ...  0.052650  0.094038  0.063539   \n4958  0.052975  0.056481  0.049994  ...  0.032762 -0.071837 -0.018743   \n4959  0.060280  0.065134  0.052870  ...  0.019050  0.006829 -0.034760   \n\n           763       764       765       766       767       768  Label  \n0    -0.044338  0.071391 -0.020000 -0.211531 -0.201435  0.224799      0  \n1    -0.031098 -0.052139 -0.169000 -0.215130 -0.104537  0.156784      0  \n2     0.190759 -0.223332 -0.239000 -0.088656 -0.331340  0.067402      0  \n3     0.064948  0.041458 -0.039900 -0.113489 -0.178424  0.154598      0  \n4     0.144202  0.093995  0.022300 -0.194535 -0.162177  0.113191      0  \n...        ...       ...       ...       ...       ...       ...    ...  \n4955  0.039278 -0.080521  0.001011 -0.169109 -0.179915  0.046537      1  \n4956  0.297538  0.045275  0.055709 -0.104880 -0.276354  0.155365      1  \n4957  0.094774 -0.020944 -0.006468 -0.192807 -0.245543  0.048387      1  \n4958  0.011795 -0.100116  0.101405 -0.145528 -0.305955  0.103323      1  \n4959  0.042489 -0.018958  0.219072 -0.089119 -0.283946  0.057813      1  \n\n[4960 rows x 813 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>768</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.058059</td>\n      <td>0.041292</td>\n      <td>0.041770</td>\n      <td>0.030273</td>\n      <td>0.023149</td>\n      <td>0.022876</td>\n      <td>0.028087</td>\n      <td>0.045512</td>\n      <td>0.053745</td>\n      <td>0.044851</td>\n      <td>...</td>\n      <td>0.042317</td>\n      <td>-0.052999</td>\n      <td>-0.140937</td>\n      <td>-0.044338</td>\n      <td>0.071391</td>\n      <td>-0.020000</td>\n      <td>-0.211531</td>\n      <td>-0.201435</td>\n      <td>0.224799</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.058708</td>\n      <td>0.051248</td>\n      <td>0.027106</td>\n      <td>0.017211</td>\n      <td>0.014248</td>\n      <td>0.013115</td>\n      <td>0.014661</td>\n      <td>0.031506</td>\n      <td>0.035307</td>\n      <td>0.031559</td>\n      <td>...</td>\n      <td>0.098308</td>\n      <td>-0.128043</td>\n      <td>-0.098877</td>\n      <td>-0.031098</td>\n      <td>-0.052139</td>\n      <td>-0.169000</td>\n      <td>-0.215130</td>\n      <td>-0.104537</td>\n      <td>0.156784</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.041497</td>\n      <td>0.041953</td>\n      <td>0.016199</td>\n      <td>0.012814</td>\n      <td>0.013885</td>\n      <td>0.017758</td>\n      <td>0.023270</td>\n      <td>0.043000</td>\n      <td>0.064817</td>\n      <td>0.048953</td>\n      <td>...</td>\n      <td>0.068439</td>\n      <td>0.000398</td>\n      <td>0.047242</td>\n      <td>0.190759</td>\n      <td>-0.223332</td>\n      <td>-0.239000</td>\n      <td>-0.088656</td>\n      <td>-0.331340</td>\n      <td>0.067402</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.040264</td>\n      <td>0.025661</td>\n      <td>0.030245</td>\n      <td>0.032230</td>\n      <td>0.035553</td>\n      <td>0.040689</td>\n      <td>0.048189</td>\n      <td>0.077196</td>\n      <td>0.083134</td>\n      <td>0.077550</td>\n      <td>...</td>\n      <td>0.020538</td>\n      <td>-0.133970</td>\n      <td>-0.055184</td>\n      <td>0.064948</td>\n      <td>0.041458</td>\n      <td>-0.039900</td>\n      <td>-0.113489</td>\n      <td>-0.178424</td>\n      <td>0.154598</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.040258</td>\n      <td>0.026491</td>\n      <td>0.028075</td>\n      <td>0.030627</td>\n      <td>0.034040</td>\n      <td>0.038932</td>\n      <td>0.048109</td>\n      <td>0.076547</td>\n      <td>0.085870</td>\n      <td>0.078065</td>\n      <td>...</td>\n      <td>0.055476</td>\n      <td>-0.027392</td>\n      <td>0.026791</td>\n      <td>0.144202</td>\n      <td>0.093995</td>\n      <td>0.022300</td>\n      <td>-0.194535</td>\n      <td>-0.162177</td>\n      <td>0.113191</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4955</th>\n      <td>0.031755</td>\n      <td>0.019953</td>\n      <td>0.024758</td>\n      <td>0.030994</td>\n      <td>0.034981</td>\n      <td>0.041137</td>\n      <td>0.056205</td>\n      <td>0.097056</td>\n      <td>0.121208</td>\n      <td>0.090960</td>\n      <td>...</td>\n      <td>0.077830</td>\n      <td>-0.032965</td>\n      <td>-0.064442</td>\n      <td>0.039278</td>\n      <td>-0.080521</td>\n      <td>0.001011</td>\n      <td>-0.169109</td>\n      <td>-0.179915</td>\n      <td>0.046537</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4956</th>\n      <td>0.029550</td>\n      <td>0.023065</td>\n      <td>0.019590</td>\n      <td>0.022693</td>\n      <td>0.025671</td>\n      <td>0.032891</td>\n      <td>0.050739</td>\n      <td>0.098898</td>\n      <td>0.133602</td>\n      <td>0.104019</td>\n      <td>...</td>\n      <td>0.060514</td>\n      <td>0.043260</td>\n      <td>-0.200303</td>\n      <td>0.297538</td>\n      <td>0.045275</td>\n      <td>0.055709</td>\n      <td>-0.104880</td>\n      <td>-0.276354</td>\n      <td>0.155365</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4957</th>\n      <td>0.056590</td>\n      <td>0.036146</td>\n      <td>0.038090</td>\n      <td>0.033783</td>\n      <td>0.030009</td>\n      <td>0.030224</td>\n      <td>0.035295</td>\n      <td>0.052807</td>\n      <td>0.056211</td>\n      <td>0.047584</td>\n      <td>...</td>\n      <td>0.052650</td>\n      <td>0.094038</td>\n      <td>0.063539</td>\n      <td>0.094774</td>\n      <td>-0.020944</td>\n      <td>-0.006468</td>\n      <td>-0.192807</td>\n      <td>-0.245543</td>\n      <td>0.048387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4958</th>\n      <td>0.055444</td>\n      <td>0.036059</td>\n      <td>0.043130</td>\n      <td>0.036509</td>\n      <td>0.030981</td>\n      <td>0.031196</td>\n      <td>0.034568</td>\n      <td>0.052975</td>\n      <td>0.056481</td>\n      <td>0.049994</td>\n      <td>...</td>\n      <td>0.032762</td>\n      <td>-0.071837</td>\n      <td>-0.018743</td>\n      <td>0.011795</td>\n      <td>-0.100116</td>\n      <td>0.101405</td>\n      <td>-0.145528</td>\n      <td>-0.305955</td>\n      <td>0.103323</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4959</th>\n      <td>0.052534</td>\n      <td>0.035702</td>\n      <td>0.041593</td>\n      <td>0.035208</td>\n      <td>0.029963</td>\n      <td>0.029941</td>\n      <td>0.036255</td>\n      <td>0.060280</td>\n      <td>0.065134</td>\n      <td>0.052870</td>\n      <td>...</td>\n      <td>0.019050</td>\n      <td>0.006829</td>\n      <td>-0.034760</td>\n      <td>0.042489</td>\n      <td>-0.018958</td>\n      <td>0.219072</td>\n      <td>-0.089119</td>\n      <td>-0.283946</td>\n      <td>0.057813</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>4960 rows × 813 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.concat([Benign_df, Malignant_df], ignore_index=True)\n",
    "data_df "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T07:22:39.665750Z",
     "start_time": "2024-04-27T07:22:39.622505Z"
    }
   },
   "id": "d96d2a7419006e24",
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "source": [
    "Virtual Adversarial Training (VAT):\n",
    "输入：预提取的特征向量及其对应的标签（有标签数据）和无标签数据的特征向量。\n",
    "方法：针对每个无标签数据点，计算其特征向量上的小幅度扰动（虚拟对抗扰动），使模型在此扰动下的预测分布与原特征向量的预测分布差异最大。然后在损失函数中加入一项正则化项，惩罚这种差异，促使模型在小扰动下的预测保持一致。\n",
    "Mean Teacher:\n",
    "输入：预提取的特征向量及其对应的标签（有标签数据）和无标签数据的特征向量。\n",
    "方法：使用两套相同的模型架构，一套是学生模型（Student），用于训练；另一套是教师模型（Teacher），其权重是学生模型权重的指数移动平均（EMA）。教师模型对无标签数据的特征向量做出预测，学生模型则在数据增强（如随机翻转、裁剪等）后的特征向量上做出预测。损失函数中包含一个一致性正则化项，要求学生模型的预测尽可能接近教师模型的预测。\n",
    "Temporal Ensembling:\n",
    "输入：预提取的特征向量及其对应的标签（有标签数据）和无标签数据的特征向量。\n",
    "方法：在训练过程中，为每个无标签数据点维护一个历史预测结果的平均（ensembling），并在每次迭代时更新这个平均。损失函数中包含一个一致性正则化项，鼓励当前模型对无标签数据特征向量的预测与该数据点的历史平均预测一致。\n",
    "MixMatch:\n",
    "输入：预提取的特征向量及其对应的标签（有标签数据）和无标签数据的特征向量。\n",
    "方法：结合数据增强、温度平滑（temperature sharpening）和一致性正则化。对无标签数据的特征向量进行多种数据增强操作，生成多个版本。模型对这些版本做出预测，然后通过温度平滑得到伪标签。损失函数中不仅包括有标签数据的交叉熵损失，还包括无标签数据的伪标签与模型对其增强版本预测结果之间的一致性损失。\n",
    "UDA (Unsupervised Data Augmentation):\n",
    "输入：预提取的特征向量及其对应的标签（有标签数据）和无标签数据的特征向量。\n",
    "方法：类似于MixMatch，但采用了更强的数据增强策略（如RandAugment）和更先进的标签平滑技术。模型在有标签数据和无标签数据（经过增强）上进行训练，损失函数中包含一致性正则化项，要求模型对同一无标签数据点的不同增强版本的预测保持一致。\n",
    "这些方法均适用于已预提取特征向量的半监督分类任务，并通过一致性正则化策略利用无标签数据。在实际应用中，可能需要根据预提取特征向量的特性和任务需求，选择合适的数据增强策略、正则化强度以及模型架构。\n",
    "\n",
    "\n",
    "我现在有一个预提取好的数据集，是dataframe格式形状为（2480*813），最后一列是标签列 请写一个适合我的数据集的Virtual Adversarial Training半监督分类代码 最好带有数据预处理过程"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbc59fdb3222e01"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树分类器的准确率: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据集\n",
    "X = data_df.iloc[:, :-1]  # 特征数据\n",
    "y = data_df.iloc[:, -1]  # 目标变量（类别）\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 创建决策树分类器\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 使用训练集训练模型\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 计算并打印准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"决策树分类器的准确率: {accuracy:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:58:47.418687Z",
     "start_time": "2024-04-28T13:58:46.634051Z"
    }
   },
   "id": "d06831c3d5c96c73",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Virtual Adversarial Training (VAT)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 1. 数据预处理\n",
    "def load_data(data):\n",
    "    data_load = data  # 替换为您的数据集路径\n",
    "    X = data_load.iloc[:, :-1].values  # 特征向量\n",
    "    y = data_load.iloc[:, -1].values  # 标签\n",
    "    # 标准化特征向量\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # 划分数据集\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(data_df) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:40.123243Z",
     "start_time": "2024-04-28T13:46:40.091393Z"
    }
   },
   "id": "ac60866cc3871f86",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 2. 定义自定义数据集类\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx] if self.y is not None else None\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        if y is not None:\n",
    "            return x, y\n",
    "        else:\n",
    "            return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:40.888936Z",
     "start_time": "2024-04-28T13:46:40.880551Z"
    }
   },
   "id": "b19538d19d0a5b94",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3. 定义模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:41.534154Z",
     "start_time": "2024-04-28T13:46:41.517834Z"
    }
   },
   "id": "b0477632382bd676",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = len(np.unique(y_train))  \n",
    "model = MLP(input_dim, hidden_dim, output_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:42.195759Z",
     "start_time": "2024-04-28T13:46:42.163304Z"
    }
   },
   "id": "e88049c7cfbc8caa",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# 4. 定义VAT损失函数\n",
    "def vat_loss(model, x, y=None, eps=1e-6, xi=1e-6):\n",
    "    x_adv = x.detach() + xi * torch.randn_like(x)\n",
    "    x_adv.requires_grad_()\n",
    "    with torch.enable_grad():\n",
    "        pred_adv = model(x_adv)\n",
    "        if y is not None:\n",
    "            loss = nn.CrossEntropyLoss()(pred_adv, y)\n",
    "        else:\n",
    "            loss = -torch.mean(torch.logsumexp(pred_adv, dim=1))\n",
    "        grad = torch.autograd.grad(loss, [x_adv])[0]\n",
    "        x_adv = x_adv.detach() + eps * torch.sign(grad)\n",
    "        x_adv = torch.clamp(x_adv, min=x.min(), max=x.max())\n",
    "    model_output = model(x)\n",
    "    pred_clean = model(x)\n",
    "    pred_adv = model(x_adv)\n",
    "    consistency_loss = nn.KLDivLoss(reduction='batchmean')(F.softmax(pred_clean, dim=1), F.softmax(pred_adv, dim=1))\n",
    "\n",
    "    if y is not None:\n",
    "        return nn.CrossEntropyLoss()(pred_clean, y) + consistency_loss\n",
    "    else:\n",
    "        return consistency_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:43.069249Z",
     "start_time": "2024-04-28T13:46:43.052039Z"
    }
   },
   "id": "2c6fe50a90355ea5",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 5. 定义训练函数\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = vat_loss(model, inputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(dataloader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:43.944751Z",
     "start_time": "2024-04-28T13:46:43.937380Z"
    }
   },
   "id": "254496389bfca147",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 6. 设置设备和数据加载器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = FeatureDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = FeatureDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:45.776578Z",
     "start_time": "2024-04-28T13:46:44.684356Z"
    }
   },
   "id": "f2dcf5f06e4f0520",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: -0.4857\n",
      "Epoch 2, Train Loss: -0.7111\n",
      "Epoch 3, Train Loss: -0.8633\n",
      "Epoch 4, Train Loss: -0.9597\n",
      "Epoch 5, Train Loss: -1.0108\n",
      "Epoch 6, Train Loss: -1.0356\n",
      "Epoch 7, Train Loss: -1.0449\n",
      "Epoch 8, Train Loss: -1.0479\n",
      "Epoch 9, Train Loss: -1.0483\n",
      "Epoch 10, Train Loss: -1.0480\n",
      "Epoch 11, Train Loss: -1.0477\n",
      "Epoch 12, Train Loss: -1.0476\n",
      "Epoch 13, Train Loss: -1.0476\n",
      "Epoch 14, Train Loss: -1.0477\n",
      "Epoch 15, Train Loss: -1.0480\n",
      "Epoch 16, Train Loss: -1.0482\n",
      "Epoch 17, Train Loss: -1.0484\n",
      "Epoch 18, Train Loss: -1.0487\n",
      "Epoch 19, Train Loss: -1.0489\n",
      "Epoch 20, Train Loss: -1.0490\n",
      "Epoch 21, Train Loss: -1.0492\n",
      "Epoch 22, Train Loss: -1.0493\n",
      "Epoch 23, Train Loss: -1.0494\n",
      "Epoch 24, Train Loss: -1.0495\n",
      "Epoch 25, Train Loss: -1.0495\n",
      "Epoch 26, Train Loss: -1.0496\n",
      "Epoch 27, Train Loss: -1.0496\n",
      "Epoch 28, Train Loss: -1.0497\n",
      "Epoch 29, Train Loss: -1.0497\n",
      "Epoch 30, Train Loss: -1.0498\n",
      "Epoch 31, Train Loss: -1.0498\n",
      "Epoch 32, Train Loss: -1.0499\n",
      "Epoch 33, Train Loss: -1.0499\n",
      "Epoch 34, Train Loss: -1.0500\n",
      "Epoch 35, Train Loss: -1.0500\n",
      "Epoch 36, Train Loss: -1.0501\n",
      "Epoch 37, Train Loss: -1.0501\n",
      "Epoch 38, Train Loss: -1.0502\n",
      "Epoch 39, Train Loss: -1.0502\n",
      "Epoch 40, Train Loss: -1.0502\n",
      "Epoch 41, Train Loss: -1.0503\n",
      "Epoch 42, Train Loss: -1.0503\n",
      "Epoch 43, Train Loss: -1.0504\n",
      "Epoch 44, Train Loss: -1.0504\n",
      "Epoch 45, Train Loss: -1.0504\n",
      "Epoch 46, Train Loss: -1.0505\n",
      "Epoch 47, Train Loss: -1.0505\n",
      "Epoch 48, Train Loss: -1.0506\n",
      "Epoch 49, Train Loss: -1.0506\n",
      "Epoch 50, Train Loss: -1.0506\n",
      "Epoch 51, Train Loss: -1.0507\n",
      "Epoch 52, Train Loss: -1.0507\n",
      "Epoch 53, Train Loss: -1.0507\n",
      "Epoch 54, Train Loss: -1.0508\n",
      "Epoch 55, Train Loss: -1.0508\n",
      "Epoch 56, Train Loss: -1.0508\n",
      "Epoch 57, Train Loss: -1.0509\n",
      "Epoch 58, Train Loss: -1.0509\n",
      "Epoch 59, Train Loss: -1.0509\n",
      "Epoch 60, Train Loss: -1.0509\n",
      "Epoch 61, Train Loss: -1.0510\n",
      "Epoch 62, Train Loss: -1.0510\n",
      "Epoch 63, Train Loss: -1.0510\n",
      "Epoch 64, Train Loss: -1.0510\n",
      "Epoch 65, Train Loss: -1.0511\n",
      "Epoch 66, Train Loss: -1.0511\n",
      "Epoch 67, Train Loss: -1.0511\n",
      "Epoch 68, Train Loss: -1.0511\n",
      "Epoch 69, Train Loss: -1.0511\n",
      "Epoch 70, Train Loss: -1.0512\n",
      "Epoch 71, Train Loss: -1.0512\n",
      "Epoch 72, Train Loss: -1.0512\n",
      "Epoch 73, Train Loss: -1.0512\n",
      "Epoch 74, Train Loss: -1.0512\n",
      "Epoch 75, Train Loss: -1.0513\n",
      "Epoch 76, Train Loss: -1.0513\n",
      "Epoch 77, Train Loss: -1.0513\n",
      "Epoch 78, Train Loss: -1.0513\n",
      "Epoch 79, Train Loss: -1.0513\n",
      "Epoch 80, Train Loss: -1.0513\n",
      "Epoch 81, Train Loss: -1.0513\n",
      "Epoch 82, Train Loss: -1.0514\n",
      "Epoch 83, Train Loss: -1.0514\n",
      "Epoch 84, Train Loss: -1.0514\n",
      "Epoch 85, Train Loss: -1.0514\n",
      "Epoch 86, Train Loss: -1.0514\n",
      "Epoch 87, Train Loss: -1.0514\n",
      "Epoch 88, Train Loss: -1.0514\n",
      "Epoch 89, Train Loss: -1.0514\n",
      "Epoch 90, Train Loss: -1.0515\n",
      "Epoch 91, Train Loss: -1.0515\n",
      "Epoch 92, Train Loss: -1.0515\n",
      "Epoch 93, Train Loss: -1.0515\n",
      "Epoch 94, Train Loss: -1.0515\n",
      "Epoch 95, Train Loss: -1.0515\n",
      "Epoch 96, Train Loss: -1.0515\n",
      "Epoch 97, Train Loss: -1.0515\n",
      "Epoch 98, Train Loss: -1.0515\n",
      "Epoch 99, Train Loss: -1.0516\n",
      "Epoch 100, Train Loss: -1.0516\n"
     ]
    }
   ],
   "source": [
    "# 7. 训练模型\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:51.447104Z",
     "start_time": "2024-04-28T13:46:47.508501Z"
    }
   },
   "id": "1d97fe0b69c930ad",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 8. 评估模型（可选）\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader, device)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:46:55.197425Z",
     "start_time": "2024-04-28T13:46:55.178067Z"
    }
   },
   "id": "6786f76a5d06c6d1",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "您提供的代码实现了一个基于虚拟对抗训练（Virtual Adversarial Training, VAT）的多层感知器（Multilayer Perceptron, MLP）模型。以下是代码各部分的简要说明：\n",
    "1. 数据预处理\n",
    "加载数据集，提取特征向量 X 和标签 y。\n",
    "使用 StandardScaler 对特征向量进行标准化。\n",
    "使用 train_test_split 函数将数据划分为训练集和测试集，保持类别比例均衡。\n",
    "2. 定义自定义数据集类 FeatureDataset\n",
    "构建一个继承自 torch.utils.data.Dataset 的类，用于封装训练和测试数据。\n",
    "存储特征向量 X 和标签 y（如果有）作为张量，并支持数据转换（如果提供）。\n",
    "实现 __len__ 方法返回数据集长度，__getitem__ 方法返回指定索引处的样本及其标签（如果有）。\n",
    "3. 定义 MLP 模型\n",
    "创建一个继承自 torch.nn.Module 的类 MLP。\n",
    "定义两个全连接层（fc1 和 fc2），分别用于特征映射和输出分类。\n",
    "在 forward 方法中，先使用 ReLU 激活函数处理 fc1 的输出，再通过 fc2 得到最终的模型输出。\n",
    "4. 定义 VAT 损失函数 vat_loss\n",
    "计算对抗扰动 x_adv，通过在输入数据 x 上添加小幅度的随机噪声并启用梯度计算。\n",
    "计算对抗样本的预测结果 pred_adv，根据是否提供标签 y，选择使用交叉熵损失或负对数似然损失。\n",
    "计算对抗样本相对于损失的梯度 grad，并据此更新对抗扰动。\n",
    "限制对抗扰动的范围，使其不超过原始输入数据的最小值和最大值。\n",
    "计算模型在原始输入 x 和对抗扰动后的输入 x_adv 上的预测结果（pred_clean 和 pred_adv）。\n",
    "使用 Kullback-Leibler 散度损失（KL divergence loss）计算一致性损失（consistency loss），衡量模型对原始输入和对抗扰动输入的预测分布差异。\n",
    "最终，根据是否提供标签 y，返回交叉熵损失加上一致性损失，或者仅返回一致性损失。\n",
    "5. 定义训练函数 train_epoch\n",
    "在给定一个 epoch 中，对模型进行训练。\n",
    "使用 model.train() 将模型置于训练模式。\n",
    "初始化累计损失 running_loss 为 0。\n",
    "遍历数据加载器中的每个批次，将数据和标签转移到指定设备上。\n",
    "对每个批次执行以下操作：\n",
    "清零优化器梯度。\n",
    "计算 VAT 损失。\n",
    "反向传播损失并更新模型参数。\n",
    "累加当前批次的损失乘以样本数量。\n",
    "返回该 epoch 的平均训练损失。\n",
    "6. 设置设备和数据加载器\n",
    "根据硬件条件选择使用 CPU 或 GPU。\n",
    "创建训练集和测试集的 FeatureDataset 实例。\n",
    "使用 DataLoader 构建训练和测试数据加载器，设置合适的批量大小和是否打乱数据。\n",
    "7. 训练模型\n",
    "将模型移动到指定设备。\n",
    "使用 Adam 优化器初始化模型参数的学习率。\n",
    "设置总的训练轮数（epochs）。\n",
    "遍历所有训练轮次，对每个轮次执行以下操作：\n",
    "调用 train_epoch 函数进行单轮训练。\n",
    "打印当前轮次的训练损失。\n",
    "综上所述，这段代码实现了基于 VAT 的 MLP 模型训练流程，包括数据预处理、自定义数据集类、模型定义、VAT 损失函数、训练函数、设备和数据加载器设置以及模型训练循环。您可以运行这段代码，根据输出的训练损失来监控模型训练过程。如果需要评估模型性能，还需在训练完成后添加相应的测试代码，计算模型在测试集上的准确率或其他评价指标。\n",
    "\n",
    "定义评估函数 evaluate\n",
    "将模型设置为评估模式（model.eval()），关闭 BatchNorm 和 Dropout 层的随机性，禁用梯度计算（with torch.no_grad():）以提高效率。\n",
    "初始化正确预测数 correct 和总样本数 total 为 0。\n",
    "遍历测试数据加载器中的每个批次：\n",
    "将数据和标签转移到指定设备上。\n",
    "通过模型计算输出 outputs。\n",
    "使用 torch.max 函数找到输出张量的最大值及其对应的索引（即预测类别），取下标 0 为预测类别。\n",
    "更新总样本数 total 为当前批次的样本数。\n",
    "计算预测类别与真实标签之间的匹配情况，并累加正确预测数 correct。\n",
    "返回正确率（correct / total），即正确预测数占总样本数的比例。\n",
    "计算并打印测试准确率\n",
    "调用 evaluate 函数，传入模型、测试数据加载器和设备，计算测试集上的准确率。\n",
    "打印测试准确率，保留四位小数。\n",
    "这段代码在模型训练完成后，能够有效地评估模型在测试集上的表现。通过调用 evaluate 函数并传入训练好的模型、测试数据加载器和设备，您可以得到模型在测试集上的准确率（Test Accuracy）。此指标可以帮助您了解模型在未见过数据上的泛化能力。如果您在训练过程中也添加了验证集，还可以用同样的方式计算验证集准确率，以便在训练过程中监控模型性能并调整超参数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef3c7ac76b870830"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
